{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sunrise-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sentence_transformers\n",
    "\n",
    "from util import parse_db, Document, read_markup_tsv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "\n",
    "records = parse_db(\"data_/0525_parsed.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polyphonic-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pretrain','rb') as fp:\n",
    "    pretrain_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "refined-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 18:54:47.177613 10656 SentenceTransformer.py:39] Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v2\n",
      "I0316 18:54:47.179576 10656 SentenceTransformer.py:43] Did not find folder distiluse-base-multilingual-cased-v2\n",
      "I0316 18:54:47.180572 10656 SentenceTransformer.py:49] Try to download model from server: https://sbert.net/models/distiluse-base-multilingual-cased-v2.zip\n",
      "I0316 18:54:47.183564 10656 SentenceTransformer.py:100] Load SentenceTransformer from folder: C:\\Users\\Ellie/.cache\\torch\\sentence_transformers\\sbert.net_models_distiluse-base-multilingual-cased-v2\n",
      "I0316 18:54:48.903966 10656 SentenceTransformer.py:124] Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model_use = sentence_transformers.SentenceTransformer('distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decreased-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from hnswlib import Index as HnswIndex\n",
    "\n",
    "def model_embed_text(model, text):\n",
    "    return model.encode([text],show_progress_bar = False)\n",
    "\n",
    "class Hnsw:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.vector_dim = 512\n",
    "        self.hnsw = HnswIndex(space='l2', dim=self.vector_dim)\n",
    "\n",
    "    def build_hnsw(self, texts):\n",
    "        n = len(texts)\n",
    "        self.hnsw.init_index(max_elements=n, ef_construction=100, M=16)\n",
    "        embeddings = np.zeros((n, self.vector_dim))\n",
    "        for i, text in enumerate(tqdm(texts)):\n",
    "            embeddings[i] = model_embed_text(self.model, text)\n",
    "        self.hnsw.add_items(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respected-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hnsw = Hnsw(model_use)\n",
    "pretrain_data.sort(key=lambda x: x.get(\"date\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "characteristic-orbit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3af4dcbf0ab4645aa9962604a8476da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "use_hnsw.build_hnsw([r[\"title\"] + ' ' + r[\"text\"] for r in pretrain_data[:120000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accredited-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "def separate_samples(markup, train_percent=0.85):\n",
    "\n",
    "    count_samples = len(markup)\n",
    "    shuffled_ids = list(range(count_samples))\n",
    "    random.shuffle(shuffled_ids)\n",
    "    train_len = round(count_samples * train_percent)\n",
    "\n",
    "    train_markup, test_markup = [], []\n",
    "\n",
    "    for i, id in enumerate(shuffled_ids):\n",
    "        if i < train_len:\n",
    "            train_markup.append(markup[id])\n",
    "        else:\n",
    "            test_markup.append(markup[id])        \n",
    "           \n",
    "    return train_markup, test_markup\n",
    "\n",
    "\n",
    "def get_data(markup, records):\n",
    "    input_samples, other_samples, qualities = [], [], []\n",
    "\n",
    "    for mrkp in markup:\n",
    "        first_url, second_url, quality = mrkp.values()\n",
    "\n",
    "        input_samples.append(url2record[first_url]['title'] + ' ' + url2record[first_url]['text'])\n",
    "        other_samples.append(url2record[second_url]['title'] + ' ' + url2record[second_url]['text'])\n",
    "        if quality == 'OK':\n",
    "            qualities.append(1)\n",
    "        else:\n",
    "            qualities.append(0)\n",
    "    return input_samples, other_samples, qualities\n",
    "\n",
    "url2record = dict()\n",
    "    \n",
    "for i, record in enumerate(records):\n",
    "    url2record[record[\"url\"]] = record\n",
    "\n",
    "markup = read_markup_tsv(\"data_/ru_clustering_0525_urls.tsv\")\n",
    "\n",
    "train_markup, test_markup = separate_samples(markup)\n",
    "train_inputs, train_others, train_qualities = get_data(train_markup, records)\n",
    "test_inputs, test_others, test_qualities  = get_data(test_markup, records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parallel-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 19:15:23.272417 10656 CrossEncoder.py:54] Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9dc0f25f434157874577692e65a4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=70.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 19:15:40.096313 10656 CrossEncoder.py:54] Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3368d4f1dd0b455db22ddcd0d571b181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=70.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 19:15:56.921633 10656 CrossEncoder.py:54] Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1229116ddda47b38859d043a01c3720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=70.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 19:16:13.738384 10656 CrossEncoder.py:54] Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80059eaa3bd4a8fb19e8bbbc39bd08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=70.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 19:16:30.689315 10656 CrossEncoder.py:54] Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acef98166f4499bb9b224907aa4ef59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=70.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name = 'bert-base-multilingual-uncased'\n",
    "\n",
    "ensemble_models_predicts = []\n",
    "\n",
    "for model_no in range(5):\n",
    "    model = CrossEncoder(f'{model_name}UNC/{model_no}')\n",
    "    ensemble_models_predicts.append(model.predict(list(zip(test_inputs, test_others))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "governing-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(preds):\n",
    "    entropies = []\n",
    "    \n",
    "    for pred in preds:\n",
    "        entropy_per_sample = - pred * np.log(pred)  - (1 - pred) * np.log(1 - pred)\n",
    "        entropies.append(entropy_per_sample)\n",
    "    \n",
    "    entropies = np.array(entropies)\n",
    "    \n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stainless-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty(predicts):\n",
    "    total = sum(predicts)/len(predicts)\n",
    "    \n",
    "    total_uncertainty = entropy(total)\n",
    "    \n",
    "    individual_entropy = []\n",
    "    \n",
    "    for model_predict in predicts:\n",
    "        \n",
    "        individual_entropy.append(entropy(model_predict))\n",
    "    \n",
    "    expected_data_uncertainty = sum(individual_entropy)/len(individual_entropy)\n",
    "    knowledge_uncertainty = total_uncertainty - expected_data_uncertainty     \n",
    "    \n",
    "    return total, total_uncertainty, expected_data_uncertainty, knowledge_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sitting-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, total_uncertainty, _, _,  = get_uncertainty(ensemble_models_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "subjective-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array(test_inputs)\n",
    "test_others = np.array(test_others)\n",
    "\n",
    "uncert_inp = test_inputs[total_uncertainty > 0.05]\n",
    "uncert_oth = test_others[total_uncertainty > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "greatest-liberal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncert_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "egyptian-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncert_queries = np.append(uncert_inp, uncert_oth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "stock-river",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncert_queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "rocky-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(queries, hard_negatives=1, soft_negatives=1):\n",
    "    samples = []\n",
    "    \n",
    "    for i in range(hard_negatives + soft_negatives):\n",
    "        samples.append([])\n",
    "\n",
    "    for row in tqdm(queries):\n",
    "        vector = model_embed_text(model_use, row)                  \n",
    "        \n",
    "        labels = list(use_hnsw.hnsw.knn_query(vector, k=20)[0][0])\n",
    "\n",
    "        for i in range(hard_negatives):\n",
    "            samples[i].append(labels[i * 2])\n",
    "\n",
    "        for i in range(soft_negatives):\n",
    "            samples[i + hard_negatives].append(random.randint(0, len(pretrain_data[:120000]) - 1))\n",
    "    \n",
    "    return sum(samples,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "commercial-journal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be10512fbea24141ad7bdae048c95fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bad_samples_indexes = get_samples(uncert_queries, 3, 2)\n",
    "\n",
    "train_oth_new_pairs = []\n",
    "\n",
    "for index in bad_samples_indexes:\n",
    "    train_oth_new_pairs.append(pretrain_data[index]['title'] + ' ' + pretrain_data[index]['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "textile-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_qual = np.zeros_like(bad_samples_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "copyrighted-intermediate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1220,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_qual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "phantom-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_uncert_inp = np.tile(uncert_queries, int(len(bad_samples_indexes)/len(uncert_queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "demanding-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = np.append(np.array(train_inputs), tile_uncert_inp)\n",
    "train_others = np.append(np.array(train_others), train_oth_new_pairs)\n",
    "train_qualities = np.append(np.array(train_qualities), new_qual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "outer-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_inputs.shape == train_others.shape == train_qualities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "intense-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryAccuracyEvaluator\n",
    "\n",
    "evaluator = CEBinaryAccuracyEvaluator(list(zip(test_inputs, test_others)), test_qualities)\n",
    "\n",
    "train_examples = [InputExample(texts=[inp, oth], label = float(qual)) for inp, oth, qual in zip(train_inputs, train_others, train_qualities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name = 'bert-base-multilingual-uncased'\n",
    "\n",
    "for model_no in range(5):\n",
    "    model = CrossEncoder(model_name, num_labels= 1)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "    \n",
    "    model.fit(train_dataloader=train_dataloader,\n",
    "             evaluator = evaluator,\n",
    "             warmup_steps = 600,\n",
    "             epochs = 9,\n",
    "             output_path =f'{model_name}UNC2_/{model_no}'\n",
    "             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
