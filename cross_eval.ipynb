{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daily-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sentence_transformers\n",
    "\n",
    "from util import parse_db, Document, read_markup_tsv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "\n",
    "records = parse_db(\"data_/0525_parsed.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "operating-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from statistics import median, mean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from purano.clusterer.metrics import calc_metrics\n",
    "\n",
    "\n",
    "def get_quality(markup, dist_matrix, records, dist_threshold, print_result=False):\n",
    "    clustering_model = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=dist_threshold,\n",
    "        linkage=\"average\",\n",
    "        affinity=\"precomputed\"\n",
    "    )\n",
    "\n",
    "    clustering_model.fit(dist_matrix)\n",
    "    labels = clustering_model.labels_\n",
    "    \n",
    "    idx2url = dict()\n",
    "    url2record = dict()\n",
    "    for i, record in enumerate(records):\n",
    "        idx2url[i] = record[\"url\"]\n",
    "        url2record[record[\"url\"]] = record\n",
    "\n",
    "    url2label = dict()\n",
    "    for i, label in enumerate(labels):\n",
    "        url2label[idx2url[i]] = label\n",
    "        \n",
    "    metrics = calc_metrics(markup, url2record, url2label)[0]\n",
    "    if print_result:\n",
    "        print(metrics)\n",
    "        print(\"Accuracy: {:.1f}\".format(metrics[\"accuracy\"] * 100.0))\n",
    "        print(\"Positives Recall: {:.1f}\".format(metrics[\"1\"][\"recall\"] * 100.0))\n",
    "        print(\"Positives Precision: {:.1f}\".format(metrics[\"1\"][\"precision\"] * 100.0))\n",
    "        print(\"Positives F1: {:.1f}\".format(metrics[\"1\"][\"f1-score\"] * 100.0))\n",
    "        print(\"Distance: \", dist_threshold)\n",
    "        sizes = list(Counter(labels).values())\n",
    "        print(\"Max cluster size: \", max(sizes))\n",
    "        print(\"Median cluster size: \", median(sizes))\n",
    "        print(\"Avg cluster size: {:.2f}\".format(mean(sizes)))\n",
    "        return\n",
    "    return metrics[\"1\"][\"f1-score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "skilled-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "def separate_samples(markup, train_percent=0.85):\n",
    "\n",
    "    count_samples = len(markup)\n",
    "    shuffled_ids = list(range(count_samples))\n",
    "    random.shuffle(shuffled_ids)\n",
    "    train_len = round(count_samples * train_percent)\n",
    "\n",
    "    train_markup, test_markup = [], []\n",
    "\n",
    "    for i, id in enumerate(shuffled_ids):\n",
    "        if i < train_len:\n",
    "            train_markup.append(markup[id])\n",
    "        else:\n",
    "            test_markup.append(markup[id])        \n",
    "           \n",
    "    return train_markup, test_markup\n",
    "\n",
    "\n",
    "def get_data(markup, records):\n",
    "    input_samples, other_samples, qualities = [], [], []\n",
    "\n",
    "    for mrkp in markup:\n",
    "        first_url, second_url, quality = mrkp.values()\n",
    "\n",
    "        input_samples.append(url2record[first_url]['title'] + ' ' + url2record[first_url]['text'])\n",
    "        other_samples.append(url2record[second_url]['title'] + ' ' + url2record[second_url]['text'])\n",
    "        if quality == 'OK':\n",
    "            qualities.append(1)\n",
    "        else:\n",
    "            qualities.append(0)\n",
    "    return input_samples, other_samples, qualities\n",
    "\n",
    "\n",
    "url2record = dict()\n",
    "\n",
    "for i, record in enumerate(records):\n",
    "    url2record[record[\"url\"]] = record\n",
    "\n",
    "\n",
    "markup = read_markup_tsv(\"data_/ru_clustering_0525_urls.tsv\")\n",
    "train_markup, test_markup = separate_samples(markup)\n",
    "train_inputs, train_others, train_qualities  = get_data(train_markup, records)\n",
    "test_inputs, test_others, test_qualities  = get_data(test_markup, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adverse-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_quality(markup, url2record):\n",
    "    quality_records = []\n",
    "    for mrkp in markup:\n",
    "        first_url, second_url, quality = mrkp.values()\n",
    "        quality_records.append(url2record[first_url])\n",
    "        quality_records.append(url2record[second_url])\n",
    "    return quality_records     \n",
    "\n",
    "valid_size = 50\n",
    "valid_records = get_data_quality(test_markup[:valid_size], url2record)\n",
    "\n",
    "temp = test_inputs[:valid_size] + test_others[:valid_size]\n",
    "\n",
    "valid_samples = []\n",
    "\n",
    "for i in temp:\n",
    "    for j in temp:\n",
    "        valid_samples.append([i,j])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "combined-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "I0303 12:45:49.910940 11940 CrossEncoder.py:54] Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = CrossEncoder('bert-base-multilingual-uncased', num_labels= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryAccuracyEvaluator\n",
    "evaluator = CEBinaryAccuracyEvaluator(list(zip(test_inputs, test_others)), test_qualities)\n",
    "\n",
    "train_examples = [InputExample(texts=[inp, oth], label = qual) for inp, oth, qual in zip(train_inputs, train_others, train_qualities)]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "         evaluator = evaluator,\n",
    "         warmup_steps = 600,\n",
    "         epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "naked-booth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3b2bd115c64a3c87278daaf180410c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=313.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(valid_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "designed-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reshape((2*valid_size, 2*valid_size))\n",
    "result = 1.0 - (result + np.transpose(result))/2.0\n",
    "\n",
    "for i in range(2*valid_size):\n",
    "    for j in range(2*valid_size):\n",
    "        if i == j:\n",
    "            res[i,j] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "published-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.46, 'recall': 1.0, 'f1-score': 0.6301369863013699, 'support': 23}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 27}, 'accuracy': 0.46, 'macro avg': {'precision': 0.23, 'recall': 0.5, 'f1-score': 0.31506849315068497, 'support': 50}, 'weighted avg': {'precision': 0.2116, 'recall': 0.46, 'f1-score': 0.28986301369863016, 'support': 50}}\n",
      "Accuracy: 46.0\n",
      "Positives Recall: 0.0\n",
      "Positives Precision: 0.0\n",
      "Positives F1: 0.0\n",
      "Distance:  0.001\n",
      "Max cluster size:  1\n",
      "Median cluster size:  1.0\n",
      "Avg cluster size: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ellie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ellie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ellie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "get_quality(test_markup[:valid_size], res, valid_records,0.001, print_result= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-stanley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
