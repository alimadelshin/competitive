{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extra-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import parse_db, Document, read_markup_tsv\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "records = parse_db(\"data_/0525_parsed.db\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.len = len(x)\n",
    "        self.data_x = x\n",
    "        self.data_y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data_x[index], self.data_y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aging-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "#Выбираем только \"OK\" разметки и делим данные на train/test\n",
    "def pick_ok_samples(markup, train_percent=0.85):\n",
    "    temp_ok = []\n",
    "    temp_bad = []\n",
    "    for mrkp in markup:\n",
    "      if mrkp['quality'] == 'OK':\n",
    "        temp_ok.append(mrkp)\n",
    "      else:\n",
    "        temp_bad.append(mrkp)\n",
    "    \n",
    "    count_samples = len(temp_ok)\n",
    "    shuffled_ids = list(range(count_samples))\n",
    "    random.shuffle(shuffled_ids)\n",
    "    train_len = round(count_samples * train_percent)\n",
    "\n",
    "    train_markup, test_markup = [], []\n",
    "\n",
    "    for i, id in enumerate(shuffled_ids):\n",
    "      if i < train_len:\n",
    "        train_markup.append(temp_ok[id])\n",
    "      else:\n",
    "        test_markup.append(temp_ok[id])\n",
    "        \n",
    "    for sample in temp_bad:\n",
    "        if len(test_markup) > 2 * (count_samples - train_len):\n",
    "           break\n",
    "        test_markup.append(sample)\n",
    "        \n",
    "    \n",
    "    return train_markup, test_markup\n",
    "\n",
    "\n",
    "def get_train_data(markup, records):\n",
    "    input_samples, pos_samples = [], []\n",
    "\n",
    "    for mrkp in markup:\n",
    "        first_url, second_url, quality = mrkp.values()\n",
    "        if quality == 'OK':\n",
    "            input_samples.append(url2record[first_url]['title'] + ' ' + url2record[first_url]['text'])\n",
    "            pos_samples.append(url2record[second_url]['title'] + ' ' + url2record[second_url]['text'])\n",
    "    \n",
    "    return input_samples, pos_samples\n",
    "\n",
    "def get_test_records(markup, records):\n",
    "    test_records = []\n",
    "    for record in records:\n",
    "      for mrkp in markup:\n",
    "        first_url, second_url, _ = mrkp.values()\n",
    "        if record['url'] == first_url or record['url'] == second_url:\n",
    "            test_records.append(record)\n",
    "    \n",
    "    return test_records\n",
    "\n",
    "\n",
    "url2record = dict()\n",
    "\n",
    "for i, record in enumerate(records):\n",
    "    url2record[record[\"url\"]] = record\n",
    "\n",
    "\n",
    "markup = read_markup_tsv(\"data_/ru_clustering_0525_urls.tsv\")\n",
    "train_markup, test_markup = pick_ok_samples(markup)\n",
    "input_samples, pos_samples = get_train_data(train_markup, records)\n",
    "test_records = get_test_records(test_markup, records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "first-being",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4382\n",
      "2191\n",
      "{'first_url': 'https://lenta.ru/news/2020/05/25/cubinka/?utm_medium=social&utm_source=telegram', 'second_url': 'https://www.kp.ru/daily/27134/4223524/', 'quality': 'OK'}\n",
      "{'first_url': 'https://www.facenews.ua/news/2020/481127/', 'second_url': 'https://368.media/2020/05/25/v-odesskoj-oblasti-blokirovali-nezakonnyj-sbyt-kontrabandnyh-sigaret-iz-postsovetskih-stran/', 'quality': 'BAD'}\n"
     ]
    }
   ],
   "source": [
    "print(len(test_records))\n",
    "print(len(test_markup))\n",
    "\n",
    "print(test_markup[0])\n",
    "print(test_markup[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "discrete-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from statistics import median, mean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from purano.clusterer.metrics import calc_metrics\n",
    "\n",
    "def get_quality(markup, embeds, records, dist_threshold, print_result=False):\n",
    "    clustering_model = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=dist_threshold,\n",
    "        linkage=\"average\",\n",
    "        affinity=\"cosine\"\n",
    "    )\n",
    "\n",
    "    clustering_model.fit(embeds)\n",
    "    labels = clustering_model.labels_\n",
    "\n",
    "    idx2url = dict()\n",
    "    url2record = dict()\n",
    "    for i, record in enumerate(records):\n",
    "        idx2url[i] = record[\"url\"]\n",
    "        url2record[record[\"url\"]] = record\n",
    "\n",
    "    url2label = dict()\n",
    "    for i, label in enumerate(labels):\n",
    "        url2label[idx2url[i]] = label\n",
    "        \n",
    "    metrics = calc_metrics(markup, url2record, url2label)[0]\n",
    "    if print_result:\n",
    "        print()\n",
    "        print(\"Accuracy: {:.1f}\".format(metrics[\"accuracy\"] * 100.0))\n",
    "        print(\"Positives Recall: {:.1f}\".format(metrics[\"1\"][\"recall\"] * 100.0))\n",
    "        print(\"Positives Precision: {:.1f}\".format(metrics[\"1\"][\"precision\"] * 100.0))\n",
    "        print(\"Positives F1: {:.1f}\".format(metrics[\"1\"][\"f1-score\"] * 100.0))\n",
    "        print(\"Distance: \", dist_threshold)\n",
    "        sizes = list(Counter(labels).values())\n",
    "        print(\"Max cluster size: \", max(sizes))\n",
    "        print(\"Median cluster size: \", median(sizes))\n",
    "        print(\"Avg cluster size: {:.2f}\".format(mean(sizes)))\n",
    "        \n",
    "        return\n",
    "    return metrics[\"1\"][\"f1-score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "standard-appendix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044aafbea6ff40e4ae07c7c755a019c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4382.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy: 94.0\n",
      "Positives Recall: 90.7\n",
      "Positives Precision: 97.2\n",
      "Positives F1: 93.8\n",
      "Distance:  0.37\n",
      "Max cluster size:  25\n",
      "Median cluster size:  2\n",
      "Avg cluster size: 2.33\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "use = sentence_transformers.SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def use_get_embedding(text, model):\n",
    "    return model.encode([text],show_progress_bar = False)\n",
    "\n",
    "def use_records_to_embeds(records, model):\n",
    "    embeddings = np.zeros((len(records), 512))\n",
    "    for i, record in enumerate(tqdm(records)):\n",
    "        embeddings[i] = use_get_embedding(record[\"title\"] + \" \" + record[\"text\"], model)\n",
    "    return embeddings\n",
    "\n",
    "train_use_embeddings = use_records_to_embeds(test_records, use)\n",
    "get_quality(test_markup, train_use_embeddings, test_records, 0.37, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "useful-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.len = len(x)\n",
    "        self.data_x = x\n",
    "        self.data_y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data_x[index], self.data_y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = CustomDataset(input_samples, pos_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qualified-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class FeedForwardTop(nn.Module):\n",
    "    def __init__(self, dropout = 0.05):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(512, 512)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.gelu = GELU()\n",
    "        self.f2 = nn.Linear(512, 512)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.gelu(self.f1(x)))\n",
    "        x = self.f2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compact-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def use_get_embedding(text, model, ff):\n",
    "    return ff(torch.tensor(model.encode([text], show_progress_bar = False))).detach().numpy()\n",
    "\n",
    "def use_records_to_embeds(records, model, ff):\n",
    "    embeddings = np.zeros((len(records), 512))\n",
    "    for i, record in enumerate(records):\n",
    "        embeddings[i] = use_get_embedding(record[\"title\"] + \" \" + record[\"text\"], model, ff)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "frozen-utility",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 20:10:15.523839 10136 SentenceTransformer.py:39] Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v2\n",
      "I0228 20:10:15.524805 10136 SentenceTransformer.py:43] Did not find folder distiluse-base-multilingual-cased-v2\n",
      "I0228 20:10:15.524805 10136 SentenceTransformer.py:49] Try to download model from server: https://sbert.net/models/distiluse-base-multilingual-cased-v2.zip\n",
      "I0228 20:10:15.526800 10136 SentenceTransformer.py:100] Load SentenceTransformer from folder: C:\\Users\\Ellie/.cache\\torch\\sentence_transformers\\sbert.net_models_distiluse-base-multilingual-cased-v2\n",
      "I0228 20:10:17.277148 10136 SentenceTransformer.py:124] Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "use = sentence_transformers.SentenceTransformer('distiluse-base-multilingual-cased-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-effects",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c59c0e42466423e8d0641b40e985dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=388.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0 lr: 0.001 batch_size: 16, dropout: 0.05, loss_on_epoch: 2.034864902496338\n",
      "F1-score: 82.35294117647058 dist: 0.172\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5373c88c24409d8ca73e61373ce52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=388.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 lr: 0.001 batch_size: 16, dropout: 0.05, loss_on_epoch: 1.9880783557891846\n",
      "F1-score: 83.45187292555713 dist: 0.194\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff8c6f3afdf40f2ba05edad377cf0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=388.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 lr: 0.001 batch_size: 16, dropout: 0.05, loss_on_epoch: 1.977678894996643\n",
      "F1-score: 83.59191349318289 dist: 0.194\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f683ada26448e4b2f39abc7dc94ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=388.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = 'cpu'\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "for lr in [0.001, 0.005, 0.01, 0.05, 0.1]:\n",
    "    for batch_size in [16, 32, 64]:\n",
    "        for dropout in [0.05, 0.15, 0.5]:\n",
    "            net = FeedForwardTop().to(device)\n",
    "\n",
    "            for p in net.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    torch.nn.init.xavier_uniform_(p)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "            \n",
    "            EPOCHES = 5\n",
    "            \n",
    "            \n",
    "            for epoch in range(EPOCHES):\n",
    "                total_loss = 0\n",
    "                net.train()\n",
    "                for i, (input_samples, pos_samples) in enumerate(tqdm(train_loader)):\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                    q_vectors = torch.tensor(use.encode(input_samples, show_progress_bar = False)).to(device)\n",
    "                    ctx_vectors = torch.tensor(use.encode(pos_samples, show_progress_bar = False)).to(device)\n",
    "            \n",
    "                    q_vectors = net(q_vectors)\n",
    "                    q_vectors = F.normalize(q_vectors, p=2, dim = -1)\n",
    "            \n",
    "                    ctx_vectors = net(ctx_vectors)\n",
    "                    ctx_vectors = F.normalize(ctx_vectors, p=2, dim = -1)\n",
    "            \n",
    "                    scores = torch.matmul(q_vectors, torch.transpose(ctx_vectors, 0, 1))\n",
    "            \n",
    "                    if len(q_vectors.size()) > 1:\n",
    "                        q_num = q_vectors.size(0)\n",
    "                        scores = scores.view(q_num, -1)\n",
    "                    \n",
    "                    softmax_scores = F.log_softmax(scores, dim=1)\n",
    "                    \n",
    "                    pos_idx_per_question = torch.tensor(list(range(q_vectors.size(0)))).to(device)\n",
    "            \n",
    "            \n",
    "                    loss = F.nll_loss(softmax_scores,\n",
    "                               pos_idx_per_question,\n",
    "                               reduction='mean')\n",
    "            \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "                    total_loss += loss.data\n",
    "                    \n",
    "                    '''\n",
    "                    if (i + 1) % 45 == 0:\n",
    "                        print(f'epoch: {(epoch + 1)} batch_idx: {(i + 1)} loss: {total_loss/45}')\n",
    "                        total_loss = 0\n",
    "                    ''' \n",
    "                    \n",
    "                net.eval()\n",
    "                test_embeds = use_records_to_embeds(test_records, use, net)\n",
    "                \n",
    "                print(f'EPOCH {epoch} lr: {lr} batch_size: {batch_size}, dropout: {dropout}, loss_on_epoch: {total_loss/len(train_loader)}')\n",
    "                \n",
    "                f1_scores = {}\n",
    "                \n",
    "                for dist in np.linspace(0.15, 0.37, 11):\n",
    "                    f1_scores[str(dist)] = get_quality(test_markup, test_embeds, test_records, dist, print_result=False)\n",
    "\n",
    "                f1_max_score = max(list(f1_scores.values()))\n",
    "                for dist in f1_scores.keys():\n",
    "                    if f1_scores[dist] == f1_max_score:\n",
    "                        print(f'F1-score: {f1_max_score * 100} dist: {dist}')\n",
    "                print()\n",
    "                    \n",
    "            print('\\n !!!!!!!!!! \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-clearing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
