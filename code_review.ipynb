{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daily-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sentence_transformers\n",
    "\n",
    "from util import parse_db, Document, read_markup_tsv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "\n",
    "records = parse_db(\"data_/0525_parsed.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "limiting-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from statistics import median, mean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from purano.clusterer.metrics import calc_metrics\n",
    "\n",
    "\n",
    "def get_quality(markup, dist_matrix, records, dist_threshold, print_result=False):\n",
    "    clustering_model = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=dist_threshold,\n",
    "        linkage=\"average\",\n",
    "        affinity=\"precomputed\"\n",
    "    )\n",
    "\n",
    "    clustering_model.fit(dist_matrix)\n",
    "    labels = clustering_model.labels_\n",
    "    print(labels)\n",
    "    idx2url = dict()\n",
    "    url2record = dict()\n",
    "    for i, record in enumerate(records):\n",
    "        idx2url[i] = record[\"url\"]\n",
    "        url2record[record[\"url\"]] = record\n",
    "\n",
    "    url2label = dict()\n",
    "    for i, label in enumerate(labels):\n",
    "        url2label[idx2url[i]] = label\n",
    "        \n",
    "    metrics = calc_metrics(markup, url2record, url2label)[0]\n",
    "    if print_result:\n",
    "        print()\n",
    "        print(\"Accuracy: {:.1f}\".format(metrics[\"accuracy\"] * 100.0))\n",
    "        print(\"Positives Recall: {:.1f}\".format(metrics[\"1\"][\"recall\"] * 100.0))\n",
    "        print(\"Positives Precision: {:.1f}\".format(metrics[\"1\"][\"precision\"] * 100.0))\n",
    "        print(\"Positives F1: {:.1f}\".format(metrics[\"1\"][\"f1-score\"] * 100.0))\n",
    "        print(\"Distance: \", dist_threshold)\n",
    "        sizes = list(Counter(labels).values())\n",
    "        print(\"Max cluster size: \", max(sizes))\n",
    "        print(\"Median cluster size: \", median(sizes))\n",
    "        print(\"Avg cluster size: {:.2f}\".format(mean(sizes)))\n",
    "        return\n",
    "    return metrics[\"1\"][\"f1-score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "skilled-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "def separate_samples(markup, train_percent=0.85):\n",
    "\n",
    "    count_samples = len(markup)\n",
    "    shuffled_ids = list(range(count_samples))\n",
    "    random.shuffle(shuffled_ids)\n",
    "    train_len = round(count_samples * train_percent)\n",
    "\n",
    "    train_markup, test_markup = [], []\n",
    "\n",
    "    for i, id in enumerate(shuffled_ids):\n",
    "        if i < train_len:\n",
    "            train_markup.append(markup[id])\n",
    "        else:\n",
    "            test_markup.append(markup[id])        \n",
    "           \n",
    "    return train_markup, test_markup\n",
    "\n",
    "\n",
    "def get_data(markup, records):\n",
    "    input_samples, other_samples, qualities = [], [], []\n",
    "\n",
    "    for mrkp in markup:\n",
    "        first_url, second_url, quality = mrkp.values()\n",
    "\n",
    "        input_samples.append(url2record[first_url]['title'] + ' ' + url2record[first_url]['text'])\n",
    "        other_samples.append(url2record[second_url]['title'] + ' ' + url2record[second_url]['text'])\n",
    "        if quality == 'OK':\n",
    "            qualities.append(1)\n",
    "        else:\n",
    "            qualities.append(0)\n",
    "    return input_samples, other_samples, qualities\n",
    "\n",
    "\n",
    "url2record = dict()\n",
    "\n",
    "for i, record in enumerate(records):\n",
    "    url2record[record[\"url\"]] = record\n",
    "\n",
    "\n",
    "markup = read_markup_tsv(\"data_/ru_clustering_0525_urls.tsv\")\n",
    "train_markup, test_markup = separate_samples(markup)\n",
    "train_inputs, train_others, train_qualities  = get_data(train_markup, records)\n",
    "test_inputs, test_others, test_qualities  = get_data(test_markup, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "invisible-karma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Киностудия Disney выпустила мультфильм о гее По сюжету парень хочет рассказать правду о своей сексуальной жизни родителям, но не может решиться. \n",
      "       Disney в партнерстве с Pixar впервые в своей истории выпустили девятиминутный мультфильм с главным героем-геем — Out, пишет  The New York Times . Главный герой Грег хочет переехать в другой город со своим партнером, но никак не может решиться на то, чтобы рассказать семье о своей сексуальной ориентации. Режиссером анимационной ленты стал Стивен Клэй Хантер, который ранее работал над мультфильмами «В поисках Немо» и «ВАЛЛ-И».\n",
      "Pixar выпустила короткометражку с главным героем геем Студия Pixar впервые в своей истории выпустила мультфильм, главным героем которого стал гей. Короткометражка называется Out, длится девять минут и рассказывает историю персонажа по имени Грег. Он не знает, как сообщить родителям о своей сексуальной ориентации. Грег собирается переехать к своему парню Мануэлю, а с переездом ему решают помочь родители. Дальше уже спойлеры, но события лихо закручиваются. Репетировать речь для родителей, к слову, Грегу помогает его собака.  Out уже доступен на стриминговом сервисе Disney+, мультфильм создан был в рамках программы SparkShort, суть которой как раз и заключается в экспериментах с новыми техниками и способами анимации, а также с иными подходами к истории. Режиссером короткометражки стал Стивен Клэй Хантер, который до этого работал над \"В поисках Немо\" и \"ВАЛЛ-И\".\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_inputs[3])\n",
    "print(test_others[3])\n",
    "print(test_qualities[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adverse-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_quality(markup, url2record):\n",
    "    quality_records = []\n",
    "    for mrkp in markup:\n",
    "        first_url, second_url, quality = mrkp.values()\n",
    "        quality_records.append(url2record[first_url])\n",
    "        quality_records.append(url2record[second_url])\n",
    "    return quality_records     \n",
    "\n",
    "valid_size = 3\n",
    "valid_records = get_data_quality(test_markup[:valid_size], url2record)\n",
    "\n",
    "temp = test_inputs[:valid_size] + test_others[:valid_size]\n",
    "\n",
    "valid_samples = []\n",
    "\n",
    "for i in temp:\n",
    "    for j in temp:\n",
    "        valid_samples.append([i,j])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "combined-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "I0303 16:54:45.211481 10360 CrossEncoder.py:54] Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = CrossEncoder('bert-base-multilingual-uncased', num_labels= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "informative-ethernet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60945a7f470041faa604d4c5c22f125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67cadaa04844242ad264ae94a6dffdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=789.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0303 16:59:15.612779 10360 CEBinaryAccuracyEvaluator.py:48] CESoftmaxAccuracyEvaluator: Evaluating the model on  dataset after epoch 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0303 16:59:29.173570 10360 CEBinaryAccuracyEvaluator.py:56] Accuracy: 95.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryAccuracyEvaluator\n",
    "evaluator = CEBinaryAccuracyEvaluator(list(zip(test_inputs, test_others)), test_qualities)\n",
    "\n",
    "train_examples = [InputExample(texts=[inp, oth], label = qual) for inp, oth, qual in zip(train_inputs, train_others, train_qualities)]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "         evaluator = evaluator,\n",
    "         warmup_steps = 600,\n",
    "         epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "naked-booth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9498903bb4348a3941fad3d594bb660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=2.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(valid_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fewer-textbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9965984  0.00215674 0.00219849 0.00216658 0.00220205 0.00221573\n",
      " 0.00220085 0.99673826 0.00212976 0.00212844 0.0025838  0.00214319\n",
      " 0.00222246 0.00225734 0.99670213 0.00213076 0.0022059  0.99646276\n",
      " 0.00228272 0.00218905 0.00216477 0.99657935 0.00217327 0.00217405\n",
      " 0.00231127 0.00336298 0.00226796 0.00225861 0.9967333  0.00229788\n",
      " 0.00231042 0.0021926  0.99509084 0.00214962 0.00220766 0.99642557]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "liberal-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9965984  0.00215674 0.00219849 0.00216658 0.00220205 0.00221573]\n",
      " [0.00220085 0.99673826 0.00212976 0.00212844 0.0025838  0.00214319]\n",
      " [0.00222246 0.00225734 0.99670213 0.00213076 0.0022059  0.99646276]\n",
      " [0.00228272 0.00218905 0.00216477 0.99657935 0.00217327 0.00217405]\n",
      " [0.00231127 0.00336298 0.00226796 0.00225861 0.9967333  0.00229788]\n",
      " [0.00231042 0.0021926  0.99509084 0.00214962 0.00220766 0.99642557]]\n"
     ]
    }
   ],
   "source": [
    "result = result.reshape((2*valid_size, 2*valid_size))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "designed-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.9978212  0.9977895  0.9977754  0.99774337 0.99773693]\n",
      " [0.9978212  0.         0.99780643 0.99784124 0.9970266  0.9978321 ]\n",
      " [0.9977895  0.99780643 0.         0.9978522  0.9977631  0.00422323]\n",
      " [0.9977754  0.99784124 0.9978522  0.         0.9977841  0.99783814]\n",
      " [0.99774337 0.9970266  0.9977631  0.9977841  0.         0.99774724]\n",
      " [0.99773693 0.9978321  0.00422323 0.99783814 0.99774724 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "result = 1.0 - (result + np.transpose(result))/2.0\n",
    "\n",
    "for i in range(2*valid_size):\n",
    "    for j in range(2*valid_size):\n",
    "        if i == j:\n",
    "            result[i,j] = 0\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "chemical-protest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(result[result>0.5]))\n",
    "print(len(result[result<0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ancient-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_model = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=0.5,\n",
    "        linkage=\"single\",\n",
    "        affinity=\"precomputed\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "rubber-december",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='precomputed', distance_threshold=0.5,\n",
       "                        linkage='single', n_clusters=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_model.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "frank-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clustering_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "miniature-anniversary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "interior-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мы дали 6 предложений на вход, 2 предложения имеют взаимную метку \"OK\" - то есть один кластер, и еще 4 предложения, которые  \n",
    "# никак к другу не относятся, в итоге как раз 5. То есть до этого момента все работает.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "inappropriate-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def calc_metrics(markup, url2record, labels):\n",
    "    not_found_count = 0\n",
    "    for record in markup:\n",
    "        first_url = record[\"first_url\"]\n",
    "        second_url = record[\"second_url\"]\n",
    "        not_found_in_labels = first_url not in labels or second_url not in labels\n",
    "        not_found_in_records = first_url not in url2record or second_url not in url2record\n",
    "        if not_found_in_labels or not_found_in_records:\n",
    "            not_found_count += 1\n",
    "            markup.remove(record)\n",
    "    if not_found_count != 0:\n",
    "        print(\"Not found {} pairs from markup\".format(not_found_count))\n",
    "\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    errors = []\n",
    "    for record in markup:\n",
    "        first_url = record[\"first_url\"]\n",
    "        second_url = record[\"second_url\"]\n",
    "        target = int(record[\"quality\"] == \"OK\")\n",
    "        prediction = int(labels[first_url] == labels[second_url])\n",
    "        first = url2record.get(first_url)\n",
    "        second = url2record.get(second_url)\n",
    "        targets.append(target)\n",
    "        predictions.append(prediction)\n",
    "        if target == prediction:\n",
    "            continue\n",
    "        errors.append({\n",
    "            \"target\": target,\n",
    "            \"prediction\": prediction,\n",
    "            \"first_url\": first_url,\n",
    "            \"second_url\": second_url,\n",
    "            \"first_title\": first[\"title\"],\n",
    "            \"second_title\": second[\"title\"],\n",
    "            \"first_text\": first[\"text\"],\n",
    "            \"second_text\": second[\"text\"]\n",
    "        })\n",
    "\n",
    "    metrics = classification_report(targets, predictions, output_dict=True)\n",
    "    return metrics, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecological-blink",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ellie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ellie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ellie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "idx2url = dict()\n",
    "url2record = dict()\n",
    "\n",
    "for i, record in enumerate(valid_records):\n",
    "    idx2url[i] = record[\"url\"]\n",
    "    url2record[record[\"url\"]] = record\n",
    "    \n",
    "url2label = dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    url2label[idx2url[i]] = label\n",
    "    \n",
    "metrics = calc_metrics(test_markup[:valid_size], url2record, url2label)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "infectious-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'accuracy': 0.6666666666666666, 'macro avg': {'precision': 0.3333333333333333, 'recall': 0.5, 'f1-score': 0.4, 'support': 3}, 'weighted avg': {'precision': 0.4444444444444444, 'recall': 0.6666666666666666, 'f1-score': 0.5333333333333333, 'support': 3}}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
